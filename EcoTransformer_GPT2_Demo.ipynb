{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZiO9TWK72R_"
      },
      "source": [
        "# StoryCloze Evaluation using L1 Attention with GPT2\n",
        "\n",
        "This notebook evaluates the performance of EcoTransformers on the [story_cloze](https://huggingface.co/datasets/lecslab/story_cloze) dataset.\n",
        "\n",
        "If possible, run this notebook on Google Colab, using an A100, High-RAM GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVlmOovr56sW"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets fsspec --quiet\n",
        "!pip uninstall transformers -y\n",
        "!pip install --no-cache-dir transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for running the test (not related to library)\n",
        "\n",
        "from torch.utils.data import (DataLoader,\n",
        "                              Dataset)\n",
        "\n",
        "from datasets import (load_dataset,\n",
        "                      DatasetDict)\n",
        "import transformers\n",
        "from transformers import (GPT2Config,\n",
        "                          GPT2Tokenizer,\n",
        "                          default_data_collator)\n",
        "\n",
        "import random"
      ],
      "metadata": {
        "id": "hPJdzhYkBAYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnVJCr7w7gkJ"
      },
      "outputs": [],
      "source": [
        "## Set up os variables for huggingface cache\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_DATASETS_CACHE\"] = \"/content/hf_dataset_cache\"\n",
        "os.environ[\"HF_HOME\"] = \"/content/hf_home\"\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing l1_distance.py\n",
        "\n",
        "The following block contains all the code from l1_distance.py, mimicking how you could use the library to modify the forward function of an existing architecture."
      ],
      "metadata": {
        "id": "-oOsTyDg-siT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.generation import GenerationMixin\n",
        "from transformers.models.gpt2.modeling_gpt2 import (GPT2Attention,\n",
        "                                                    GPT2MLP,\n",
        "                                                    GPT2Block,\n",
        "                                                    GPT2SequenceSummary,\n",
        "                                                    GPT2PreTrainedModel,\n",
        "                                                    GPT2Model,\n",
        "                                                    GPT2LMHeadModel,\n",
        "                                                    GPT2PreTrainedModel,\n",
        "                                                    GPT2ForSequenceClassification\n",
        "                                                    )\n",
        "from transformers.pytorch_utils import Conv1D\n",
        "\n",
        "from typing import Callable, Optional, Tuple, Union\n",
        "\n",
        "from transformers.utils import logging\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, Optional, Tuple, Union\n",
        "\n",
        "from transformers.cache_utils import Cache, EncoderDecoderCache\n",
        "from transformers.generation import GenerationMixin\n",
        "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
        "from transformers.pytorch_utils import Conv1D\n",
        "from transformers.utils import logging"
      ],
      "metadata": {
        "id": "bH-pGermA47Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNkbjbs57m9A"
      },
      "outputs": [],
      "source": [
        "# Library functions\n",
        "\n",
        "MAX_LENGTH=128\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "def generate_tensor_c(A, B):\n",
        "  \"\"\"\n",
        "  Generates tensor C based on the given formula.\n",
        "\n",
        "  Args:\n",
        "    A: A torch tensor of shape (i, j, k, d).\n",
        "    B: A torch tensor of shape (i, j, l, d).\n",
        "\n",
        "  Returns:\n",
        "    A torch tensor C of shape (i, k, l).\n",
        "  \"\"\"\n",
        "  # Get dimensions of A and B\n",
        "  i_dim, j_dim, k_dim, d_dim = A.shape\n",
        "  difference = torch.abs(A.unsqueeze(3) - B.unsqueeze(2)) # Unsqueeze adds a dimension\n",
        "\n",
        "  # Sum over the last dimension (d)\n",
        "  C = torch.sum(difference, dim=-1)\n",
        "\n",
        "  return C\n",
        "\n",
        "def generate_tensor_c_batch(A, B, batch_size=8):\n",
        "\n",
        "  # Get dimensions of A and B\n",
        "  i_dim, j_dim, k_dim, d_dim = A.shape\n",
        "  l_dim = B.shape[2]\n",
        "\n",
        "  result = torch.empty(i_dim, j_dim, k_dim, l_dim, device=A.device, dtype=A.dtype)\n",
        "\n",
        "  for i in range(0, k_dim, batch_size):\n",
        "    end = min(i + batch_size, k_dim)\n",
        "    A_batch = A[:,:,i:end,:]  # shape (i,j, batch, d)\n",
        "    difference = torch.abs(A_batch[:, :, :, None, :] - B[:, :, None, :, :]) # (i, j, batch, l, d)\n",
        "    dist_batch = torch.sum(difference, dim=-1)\n",
        "    result[:, :, i:end, :] = dist_batch\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "def L1_eager_attention_forward(module, query, key, value, attention_mask, head_mask=None, **kwargs):\n",
        "\n",
        "    \"\"\"\n",
        "    Performs forward for eager attention using L1 normalization.\n",
        "    \"\"\"\n",
        "\n",
        "    attn_weights = generate_tensor_c_batch(query, key) * (-1)\n",
        "    #attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
        "\n",
        "    if module.scale_attn_weights:\n",
        "        attn_weights = attn_weights / torch.full(\n",
        "            [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n",
        "        )\n",
        "\n",
        "    # Layer-wise attention scaling\n",
        "    if module.scale_attn_by_inverse_layer_idx:\n",
        "        attn_weights = attn_weights / float(module.layer_idx + 1)\n",
        "\n",
        "    ###causal mask is always used in decoder only transformer\n",
        "    ####GPT-2 doesn't use encoder-decoder attention (unlike T5 or BART).\n",
        "    ####It uses self-attention, meaning each token attends within the same input sequence.\n",
        "    ####Therefore, query, key, and value vectors are all computed from the same sequence â€” making their lengths equal.\n",
        "\n",
        "    if not module.is_cross_attention:\n",
        "        # if only \"normal\" attention layer implements causal mask\n",
        "        query_length, key_length = query.size(-2), key.size(-2)\n",
        "        causal_mask = module.bias[:, :, key_length - query_length : key_length, :key_length]\n",
        "        mask_value = torch.finfo(attn_weights.dtype).min\n",
        "        # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
        "        # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
        "        mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
        "        attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
        "\n",
        "    if attention_mask is not None:\n",
        "      # Apply the attention mask\n",
        "      causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n",
        "      attn_weights = attn_weights + causal_mask\n",
        "\n",
        "    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "\n",
        "    # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
        "    attn_weights = attn_weights.type(value.dtype)\n",
        "    attn_weights = module.attn_dropout(attn_weights)\n",
        "\n",
        "    # Mask heads if we want to\n",
        "    if head_mask is not None:\n",
        "        attn_weights = attn_weights * head_mask\n",
        "\n",
        "    attn_output = torch.matmul(attn_weights, value)\n",
        "    attn_output = attn_output.transpose(1, 2)\n",
        "\n",
        "    return attn_output, attn_weights\n",
        "\n",
        "\n",
        "# Patch in model\n",
        "class L1GPT2Attention(GPT2Attention):\n",
        "\n",
        "    def __init__(self, config, layer_idx=None, is_cross_attention=False):\n",
        "\n",
        "        # Inherit from GPT2Attention\n",
        "        super().__init__(config)\n",
        "\n",
        "        # Completely change the init function (for safety)\n",
        "        self.config = config\n",
        "\n",
        "        max_positions = config.max_position_embeddings\n",
        "        self.register_buffer(\n",
        "            \"bias\",\n",
        "            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n",
        "                1, 1, max_positions, max_positions\n",
        "            ),\n",
        "            persistent=False,\n",
        "        )\n",
        "        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n",
        "\n",
        "        self.embed_dim = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        self.split_size = self.embed_dim\n",
        "        if self.head_dim * self.num_heads != self.embed_dim: # Corrected self.nit to self.num_heads\n",
        "            raise ValueError(\n",
        "                f\"`embed_dim` must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
        "                f\" {self.num_heads}).\"\n",
        "            )\n",
        "\n",
        "        self.scale_attn_weights = config.scale_attn_weights\n",
        "        self.is_cross_attention = is_cross_attention\n",
        "\n",
        "        # Layer-wise attention scaling, reordering, and upcasting\n",
        "        self.scale_attn_by_inverse_layer_idx = config.scale_attn_by_inverse_layer_idx\n",
        "        self.layer_idx = layer_idx\n",
        "        self.reorder_and_upcast_attn = config.reorder_and_upcast_attn\n",
        "\n",
        "\n",
        "        self.c_attn = Conv1D(3 * self.embed_dim, self.embed_dim) # mapping for the key, value, and l matrix (utility matrix)\n",
        "        self.q_attn = Conv1D(self.embed_dim, self.embed_dim) # mapping for the query matrix\n",
        "\n",
        "        self.c_proj = Conv1D(self.embed_dim, self.embed_dim)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.is_causal = True\n",
        "\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    # Moved forward method outside of __init__\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
        "        past_key_value: Optional[Cache] = None,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n",
        "        # FIX THIS LATER\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "        if is_cross_attention:\n",
        "            if not hasattr(self, \"q_attn\"):\n",
        "                raise ValueError(\n",
        "                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n",
        "                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n",
        "                )\n",
        "\n",
        "            query_states = self.q_attn(hidden_states)\n",
        "            key_states, value_states = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2)\n",
        "\n",
        "        shape_q = (*query_states.shape[:-1], -1, self.head_dim)\n",
        "        shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n",
        "\n",
        "        query_states = query_states.view(shape_q).transpose(1, 2)\n",
        "        key_states = key_states.view(shape_kv).transpose(1, 2)\n",
        "        value_states = value_states.view(shape_kv).transpose(1, 2)\n",
        "\n",
        "        if past_key_value is not None:\n",
        "            if isinstance(past_key_value, EncoderDecoderCache):\n",
        "                if is_cross_attention:\n",
        "                    past_key_value = past_key_value.cross_attention_cache\n",
        "                else:\n",
        "                    past_key_value = past_key_value.self_attention_cache\n",
        "            cache_kwargs = {\"cache_position\": cache_position}\n",
        "            key_states, value_states = past_key_value.update(\n",
        "                key_states, value_states, self.layer_idx, cache_kwargs=cache_kwargs\n",
        "            )\n",
        "\n",
        "        is_causal = attention_mask is None and query_states.shape[-2] > 1 and not is_cross_attention\n",
        "\n",
        "        using_eager = self.config._attn_implementation == \"eager\"\n",
        "\n",
        "        # Define the new attention_interface as the custom L1_eager_attention_forward\n",
        "        attention_interface: Callable = L1_eager_attention_forward\n",
        "\n",
        "        # XM: changing this to always be eager\n",
        "        # if self.config._attn_implementation != \"eager\":\n",
        "        #     if self.config._attn_implementation == \"sdpa\" and (output_attentions or head_mask is not None):\n",
        "\n",
        "        using_eager = True\n",
        "        logger.warning_once(\n",
        "            \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
        "            'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
        "        )\n",
        "            # else:\n",
        "            #     print(\"else\")\n",
        "            #     # Attention functions are consistent with previous equivalent attention classes, however they do not support some options\n",
        "            #     # (e.g. layer scaling, head mask) that eager supports. These implementations are thus equivalent to previous code, but\n",
        "            #     # not necessarily to eager (if mentioned options are provided).\n",
        "            #     attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
        "\n",
        "        if using_eager and self.reorder_and_upcast_attn:\n",
        "            # using eager\n",
        "            print(\"using eager_attention_forward\")\n",
        "            attn_output, attn_weights = self._upcast_and_reordered_attn(\n",
        "                query_states, key_states, value_states, attention_mask, head_mask\n",
        "            )\n",
        "        else:\n",
        "            dropout = kwargs.pop('dropout', self.attn_dropout.p if self.training else 0.0)\n",
        "            attn_output, attn_weights = attention_interface(\n",
        "                self,\n",
        "                query=query_states,\n",
        "                key=key_states,\n",
        "                value=value_states,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                dropout=dropout,\n",
        "                is_causal=is_causal,\n",
        "                **kwargs,\n",
        "            )\n",
        "\n",
        "        attn_output = attn_output.reshape(*attn_output.shape[:-2], -1).contiguous()\n",
        "        attn_output = self.c_proj(attn_output)\n",
        "        attn_output = self.resid_dropout(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "\n",
        "# Create L1 \"clones\" inheriting from GPT2 equivalents to patch in our changes to the forward function.\n",
        "\n",
        "class L1GPT2Block(GPT2Block):\n",
        "\n",
        "    def __init__(self, config, layer_idx=None):\n",
        "        super().__init__(config, layer_idx=layer_idx)\n",
        "        self.attn = L1GPT2Attention(config,\n",
        "                                        layer_idx=layer_idx)\n",
        "\n",
        "class L1GPT2Model(GPT2Model):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.h = nn.ModuleList([L1GPT2Block(config, layer_idx=i)\n",
        "        for i in range(config.num_hidden_layers)])\n",
        "\n",
        "\n",
        "class L1GPT2LMHeadModel(GPT2PreTrainedModel, GenerationMixin):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.transformer = L1GPT2Model(config)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "\n",
        "class L1GPT2ForSequenceClassification(GPT2ForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.transformer = L1GPT2Model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLGJ44VcLMYJ"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr72pWte7yYS"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-TRGPVWfPqw"
      },
      "source": [
        "# Processing the dataset\n",
        "\n",
        "The dataset is in the following form:\n",
        "\n",
        "```\n",
        "{\n",
        "  'prompt': 'James was very low on money and needed food. So he decided to count his pennies and see what he came up with. He came up with three dollars and eight cents. But then he remembered he had a visa gift card from his birthday!',\n",
        "  'chosen': 'James was then able to buy himself food.',\n",
        "  'rejected': 'James did not have any money.'\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQIeHhXz8I_8"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Dataset\n",
        "\n",
        "We create a custom Dataset class to help organize the dataset for training."
      ],
      "metadata": {
        "id": "1BITlFvz_cWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y98jdlkIt7m2"
      },
      "outputs": [],
      "source": [
        "class StoryClozeDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_length=MAX_LENGTH):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset: A HuggingFace Dataset object\n",
        "            tokenizer: HuggingFace tokenizer\n",
        "            max_length: maximum sequence length to truncate the input\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve the prompt, chosen candidate, and rejected candidate\n",
        "        prompt = self.dataset[idx]['prompt']\n",
        "        chosen = self.dataset[idx]['chosen']\n",
        "        rejected = self.dataset[idx]['rejected']\n",
        "\n",
        "        # The correct candidate is 'chosen', and the incorrect candidate is 'rejected'\n",
        "        label_chosen = 0  # 'chosen' is the correct continuation\n",
        "        label_rejected = 1  # 'rejected' is the incorrect continuation\n",
        "\n",
        "        # Tokenize prompt and candidates (chosen and rejected)\n",
        "        input_chosen = prompt + \" \" + chosen\n",
        "        input_rejected = prompt + \" \" + rejected\n",
        "\n",
        "        # Tokenize both inputs\n",
        "        encoding_chosen = self.tokenizer(input_chosen,\n",
        "                                         truncation=True,\n",
        "                                         padding=\"max_length\",\n",
        "                                         max_length=self.max_length,\n",
        "                                         return_tensors=\"pt\")\n",
        "        encoding_rejected = self.tokenizer(input_rejected,\n",
        "                                           truncation=True,\n",
        "                                           padding=\"max_length\",\n",
        "                                           max_length=self.max_length,\n",
        "                                           return_tensors=\"pt\")\n",
        "\n",
        "        if random.random() > 0.5:\n",
        "          # Combine both candidates into the same batch\n",
        "          # Do not flatten here; let the data collator handle batching and padding\n",
        "          input_ids = torch.cat([encoding_chosen[\"input_ids\"], encoding_rejected[\"input_ids\"]], dim=0)\n",
        "          attention_mask = torch.cat([encoding_chosen[\"attention_mask\"], encoding_rejected[\"attention_mask\"]], dim=0)\n",
        "\n",
        "          # Combine labels (0 for chosen and 1 for rejected)\n",
        "          # Keep labels as a tensor of shape [2] for the two candidates\n",
        "          labels = torch.tensor([label_chosen, label_rejected], dtype=torch.long)\n",
        "        else:\n",
        "          # Combine both candidates into the same batch\n",
        "          # Do not flatten here; let the data collator handle batching and padding\n",
        "          input_ids = torch.cat([encoding_rejected[\"input_ids\"], encoding_chosen[\"input_ids\"]], dim=0)\n",
        "          attention_mask = torch.cat([encoding_rejected[\"attention_mask\"], encoding_chosen[\"attention_mask\"]], dim=0)\n",
        "\n",
        "          # Combine labels (0 for chosen and 1 for rejected)\n",
        "          # Keep labels as a tensor of shape [2] for the two candidates\n",
        "          labels = torch.tensor([label_rejected, label_chosen], dtype=torch.long)\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids, # Return unflattened tensor\n",
        "            'attention_mask': attention_mask, # Return unflattened tensor\n",
        "            'labels': labels\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxB4T6Cj88ja"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache/huggingface\n",
        "!rm -rf /content/hf_dataset_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTMvSfsV-bpo"
      },
      "outputs": [],
      "source": [
        "data = load_dataset(\"lecslab/story_cloze\", cache_dir=\"/content/hf_dataset_cache\", download_mode=\"force_redownload\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVwr68mr0bMt"
      },
      "outputs": [],
      "source": [
        "dataset = StoryClozeDataset(data, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksa_fo372J3G"
      },
      "outputs": [],
      "source": [
        "dataset.dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNNjFNepFZ_Z"
      },
      "source": [
        "```\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['prompt', 'chosen', 'rejected'],\n",
        "        num_rows: 2806\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['prompt', 'chosen', 'rejected'],\n",
        "        num_rows: 468\n",
        "    })\n",
        "    eval: Dataset({\n",
        "        features: ['prompt', 'chosen', 'rejected'],\n",
        "        num_rows: 468\n",
        "    })\n",
        "})\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XTpw_rNosDh"
      },
      "outputs": [],
      "source": [
        "train_data = StoryClozeDataset(dataset.dataset[\"train\"], tokenizer)\n",
        "test_data = StoryClozeDataset(dataset.dataset[\"test\"], tokenizer)\n",
        "val_data = StoryClozeDataset(dataset.dataset[\"eval\"], tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak9Hwdc12_Z_"
      },
      "outputs": [],
      "source": [
        "## Preprocess the data and create data_loaders\n",
        "\n",
        "# Use default_data_collator for padding and batching\n",
        "data_collator = default_data_collator\n",
        "\n",
        "# Create the DataLoader with the correct collator\n",
        "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
        "val_loader = DataLoader(val_data, batch_size=4, collate_fn=data_collator)\n",
        "test_loader = DataLoader(test_data, batch_size=4, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpcr0Xe8wagY"
      },
      "outputs": [],
      "source": [
        "config = GPT2Config(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    n_positions=MAX_LENGTH, # down from 1024\n",
        "    n_embd=MAX_LENGTH, # down from 768\n",
        "    n_layer=12,\n",
        "    n_head=4,\n",
        "    use_sdpa=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S_ACE-UB6V1"
      },
      "source": [
        "## Use Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fDLVsh-V8sy"
      },
      "outputs": [],
      "source": [
        "model = L1GPT2ForSequenceClassification(config) # 22M parameters\n",
        "print(model.num_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bzJIb2Op5-u"
      },
      "outputs": [],
      "source": [
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOWnzWxU2OV1"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4) # change from 5e-4 to 5e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2wwkJf1gaz1"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFReMNfm20LM"
      },
      "outputs": [],
      "source": [
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmFLAGX9N_F6"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, optimizer, device, num_epochs=1):\n",
        "    \"\"\"\n",
        "    Train the model on the given training data by flattening the batch dimensions.\n",
        "\n",
        "    Args:\n",
        "        model: The model to be trained.\n",
        "        train_dataloader: DataLoader for the training data.\n",
        "        optimizer: Optimizer (e.g., AdamW).\n",
        "        device: The device to train on (CPU or GPU).\n",
        "        num_epochs: The number of training epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        # Wrap train_dataloader with tqdm for progress visualization\n",
        "        for batch in train_dataloader:\n",
        "            # Move batch to the correct device (GPU or CPU)\n",
        "            input_ids = batch['input_ids'].to(device) # Shape: [batch_size, 2, seq_len]\n",
        "            attention_mask = batch['attention_mask'].to(device) # Shape: [batch_size, 2, seq_len]\n",
        "            labels = batch['labels'].to(device) # Shape: [batch_size, 2]\n",
        "\n",
        "            # Flatten the second dimension (dim=1) to combine chosen and rejected into the batch dimension\n",
        "            batch_size, num_choices, seq_len = input_ids.shape\n",
        "            input_ids = input_ids.view(batch_size * num_choices, seq_len) # Shape: [batch_size * 2, seq_len]\n",
        "            attention_mask = attention_mask.view(batch_size * num_choices, seq_len) # Shape: [batch_size * 2, seq_len]\n",
        "\n",
        "            # Flatten the labels accordingly. The labels are [0, 1] for each original story,\n",
        "            # corresponding to the chosen and rejected continuations. Flattening maintains this order.\n",
        "            labels = labels.view(batch_size * num_choices) # Shape: [batch_size * 2]\n",
        "\n",
        "            # FIX\n",
        "            max_len = MAX_LENGTH\n",
        "            input_ids = input_ids[:, :max_len]\n",
        "            attention_mask = attention_mask[:, :max_len]\n",
        "            #labels = labels[:, :max_len] # This line was commented out but could cause issues if active\n",
        "            #position_ids = position_ids[:, :max_len]\n",
        "            # FIX\n",
        "\n",
        "            # Pass labels to the model to compute loss internally\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Retrieve the loss calculated within the model's forward pass\n",
        "            logits = outputs.logits  # shape: [batch_size * 2, 2]\n",
        "            loss = loss_fn(logits, labels)  # labels shape: [batch_size * 2]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients to prevent explosion\n",
        "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print the average loss for the epoch\n",
        "        avg_epoch_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_dataloader=train_loader, optimizer=optimizer, device=device, num_epochs=epochs)"
      ],
      "metadata": {
        "id": "9SILhgtaYAT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, eval_dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the given evaluation data.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        eval_dataloader: DataLoader for the evaluation data.\n",
        "        device: The device to evaluate on (CPU or GPU).\n",
        "    \"\"\"\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for batch in eval_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)     # [batch_size, 2, seq_len]\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)           # [batch_size, 2]\n",
        "\n",
        "            batch_size, num_choices, seq_len = input_ids.shape\n",
        "            input_ids = input_ids.view(batch_size * num_choices, seq_len)\n",
        "            attention_mask = attention_mask.view(batch_size * num_choices, seq_len)\n",
        "            labels = labels.view(batch_size * num_choices)\n",
        "\n",
        "            input_ids = input_ids[:, :MAX_LENGTH]\n",
        "            attention_mask = attention_mask[:, :MAX_LENGTH]\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits  # [batch_size * 2, 2]\n",
        "\n",
        "            loss = loss_fn(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predicted class (index of max logit)\n",
        "            preds = torch.argmax(logits, dim=1)  # [batch_size * 2]\n",
        "\n",
        "            # Count correct predictions\n",
        "            correct = (preds == labels).sum().item()\n",
        "            total_correct += correct\n",
        "            total_examples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(eval_dataloader)\n",
        "    accuracy = total_correct / total_examples\n",
        "\n",
        "    print(f\"Evaluation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "MgHigVXvm_N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "id": "4L0kN7nvnAS9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}